{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fe0d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directions\n",
    "\n",
    "#make sure you have installed cv2 with command: \n",
    "#  pip install opencv-python\n",
    "#download the file called pose_iter_584000.caffemodel which is saved in this dropbox:\n",
    "#  https://www.dropbox.com/s/3x0xambj2rkyrap/pose_iter_584000.caffemodel?dl=0\n",
    "#download the file called pose_deploy.prototxt which is saved here:\n",
    "#  https://github.com/CMU-Perceptual-Computing-Lab/openpose/tree/master/models/pose/body_25\n",
    "\n",
    "#save both of these files in the same folder as this ipynb\n",
    "#save a video in the same folder or subfolder\n",
    "\n",
    "\n",
    "#issues:\n",
    "\n",
    "# the model keeps recognizing exercise equipment in the background as body parts,\n",
    "# i think using a different model(for multiple people simultaneously), \n",
    "# or adjusting confidence could help with this\n",
    "\n",
    "# i could only make the scaling work by cropping the images to squares first\n",
    "# not a big deal now with our training data, but it could cause problems in the future\n",
    "\n",
    "# i think most of these .avi videos are 30 fps. if not, framestep() will need to be updated\n",
    "\n",
    "# this script does not save the video, it only displays it in a popup window. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e598c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec84bc49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "511136c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fps: 30\n",
      "\n",
      "total frames to be analyzed:  20.68\n",
      "starting frame:  0\n",
      "starting frame:  1\n",
      "starting frame:  2\n",
      "starting frame:  3\n",
      "starting frame:  4\n",
      "starting frame:  5\n",
      "starting frame:  6\n",
      "starting frame:  7\n",
      "starting frame:  8\n",
      "starting frame:  9\n",
      "starting frame:  10\n",
      "starting frame:  11\n",
      "starting frame:  12\n",
      "starting frame:  13\n",
      "starting frame:  14\n",
      "starting frame:  15\n",
      "starting frame:  16\n",
      "starting frame:  17\n",
      "starting frame:  18\n",
      "starting frame:  19\n",
      "starting frame:  20\n",
      "(1080, 1080)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "import csv\n",
    "\n",
    "# Open the video file\n",
    "video_path = 'DeepSquat1 (1).avi'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "\n",
    "# Define the keypoint mapping for this OpenPose body_25 model\n",
    "keypoints_mapping = {\n",
    "    0:  \"Nose\", 1:  \"Neck\", 2:  \"RShoulder\", 3:  \"RElbow\", 4:  \"RWrist\", 5:  \"LShoulder\", 6:  \"LElbow\",\n",
    "    7:  \"LWrist\", 8:  \"MidHip\", 9:  \"RHip\", 10: \"RKnee\", 11: \"RAnkle\", 12: \"LHip\", 13: \"LKnee\",\n",
    "    14: \"LAnkle\", 15: \"REye\", 16: \"LEye\", 17: \"REar\", 18: \"LEar\", 19: \"LBigToe\", 20: \"LSmallToe\",\n",
    "    21: \"LHeel\", 22: \"RBigToe\", 23: \"RSmallToe\", 24: \"RHeel\"\n",
    "}\n",
    "#load the model\n",
    "net = cv2.dnn.readNetFromCaffe('pose_deploy.prototxt', 'pose_iter_584000.caffemodel')\n",
    "\n",
    "#Write frame information of joint locations into a csv file\n",
    "f = open(\"DeepSquatJoint.csv\", 'w', newline='')\n",
    "w = csv.writer(f)\n",
    "#for keypoint, label in keypoints_mapping:\n",
    "    #w.write(str(keypoints_mapping.items())\n",
    "#w.writerow(keypoints_mapping.values())\n",
    " \n",
    "def framestep(cap):\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    print(\"fps: %d\" %(fps))\n",
    "    print()\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    new_vid = []\n",
    "    if (fps % 10 == 0): #we can easily do 10 fps\n",
    "        stepsize = fps/1.2 # reset to 10\n",
    "        print(\"total frames to be analyzed: \", frame_count / stepsize)\n",
    "        curr_frame = 0\n",
    "        i = 0\n",
    "        while curr_frame < frame_count:\n",
    "            ret, frame = cap.read()\n",
    "            if curr_frame % stepsize == 0:\n",
    "                print(\"starting frame: \", i)\n",
    "                new_vid.append(pose(frame))\n",
    "                i+=1\n",
    "            curr_frame+=1\n",
    "    \n",
    "    else:\n",
    "        print(\"NOT BUILT YET\")\n",
    "        pass\n",
    "    \n",
    "    return new_vid\n",
    "    \n",
    "def squarify(frame):\n",
    "    height, width, _ = frame.shape\n",
    "    min_dim = min(height, width)\n",
    "\n",
    "    # Calculate the cropping dimensions\n",
    "    crop_height = (height - min_dim) // 2\n",
    "    crop_width = (width - min_dim) // 2\n",
    "\n",
    "    # Crop the image equally from both sides to make it a square\n",
    "    return frame[crop_height:crop_height+min_dim, crop_width:crop_width+min_dim] , min_dim\n",
    "\n",
    "def pose(frame):\n",
    "    \n",
    "    frame, size = squarify(frame)\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255, (size, size),\n",
    "                             (0, 0, 0), swapRB=False, crop=True)\n",
    "\n",
    "    # run forward pass to get the pose estimation\n",
    "    net.setInput(blob)\n",
    "    output = net.forward()\n",
    "    \n",
    "    # Extract joint locations\n",
    "    joint_locations = []\n",
    "    csv_joint_locations = []\n",
    "\n",
    "    for i in range(len(keypoints_mapping)): #-1 bc we dont want point for the background\n",
    "        keypoint = output[0, i, :, :]\n",
    "        min_val, confidence, min_loc, point = cv2.minMaxLoc(keypoint)\n",
    "\n",
    "        #if confidence > 0.1:  # can adjust the confidence threshold if needed ???\n",
    "        joint_locations.append((8 * int(point[0]), 8 * int(point[1]), 0)) #for testing/human readable\n",
    "        csv_joint_locations.append(8 * int(point[0])) #to be printed into csv\n",
    "        csv_joint_locations.append(8 * int(point[1])) #to be printed into csv\n",
    "        csv_joint_locations.append(0) #to be printed into csv\n",
    "        '''else:\n",
    "            joint_locations.append(None) #for testing/human readable\n",
    "            csv_joint_locations.append(8 * int(point[0])) #to be printed into csv\n",
    "            csv_joint_locations.append(8 * int(point[1])) #to be printed into csv\n",
    "        '''\n",
    "    #joint_locations contains the locations of the detected joints and corresponding index\n",
    "\n",
    "    '''for location in joint_locations:\n",
    "        if location:\n",
    "            x, y, index = location\n",
    "            cv2.circle(frame, (x, y), 5, (0, 0, 255), -1)\n",
    "            #cv2.putText(image, str(index), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "    '''        \n",
    "    #print joint locations into a csv file\n",
    "    w.writerow(csv_joint_locations)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "\n",
    "#write the information + circles onto each frame and pop up a window for each frame\n",
    "my_video = framestep(cap)\n",
    "size = my_video[0].shape[1], my_video[0].shape[0]\n",
    "print(size)\n",
    "out = cv2.VideoWriter(\"real.avi\", cv2.VideoWriter_fourcc(*'DIVX'), 30, size)\n",
    "\n",
    "'''for i in range(len(my_video)):\n",
    "    out.write(my_video[i])\n",
    "    cv2.imshow(\"video\", my_video[i])\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "'''\n",
    "out.release()\n",
    "cap.release()\n",
    "f.close()\n",
    "\n",
    "exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26ad567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to write the frames into a video format for viewer presentation\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "def test():\n",
    "    size = (1080, 1080)\n",
    "    out = cv2.VideoWriter(\"deep.avi\", cv2.VideoWriter_fourcc(*'DIVX'), 30, size)\n",
    "    for i in range(0, 200):\n",
    "        f, s = squarify(cap.read()[1])\n",
    "        s\n",
    "        out.write(f)\n",
    "    \n",
    "    out.release()\n",
    "\n",
    "test()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c9f3f2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[[['504', '344', '504', '392', '464', '392', '424', '352', '768', '584', '552', '384', '616', '312', '656', '240', '504', '536', '472', '528', '456', '648', '400', '832', '536', '536', '552', '640', '400', '832', '488', '328', '512', '328', '480', '336', '528', '328', '', '', '', '', '400', '832', '', '', '912', '712', '400', '832'], ['504', '568', '504', '592', '448', '592', '376', '552', '336', '472', '552', '584', '632', '544', '672', '456', '504', '736', '464', '728', '392', '736', '400', '848', '536', '736', '600', '728', '608', '848', '488', '560', '512', '560', '472', '552', '528', '552', '656', '872', '656', '864', '600', '856', '360', '872', '360', '864', '408', '856']], [['504', '328', '504', '376', '456', '376', '384', '296', '360', '248', '552', '376', '624', '312', '664', '240', '504', '544', '472', '544', '440', '680', '400', '848', '536', '544', '560', '664', '608', '840', '496', '320', '520', '320', '480', '328', '536', '320', '656', '872', '656', '872', '600', '856', '352', '872', '352', '872', '408', '856'], ['512', '528', '504', '552', '456', '552', '384', '512', '344', '432', '560', '552', '640', '496', '688', '432', '504', '696', '472', '696', '392', '728', '400', '848', '544', '696', '616', '728', '608', '848', '496', '512', '520', '512', '480', '512', '536', '512', '648', '872', '648', '872', '600', '856', '352', '872', '352', '872', '408', '856']], [['504', '384', '504', '424', '456', '416', '376', '352', '352', '272', '560', '416', '632', '368', '960', '672', '504', '592', '472', '592', '392', '712', '400', '848', '544', '592', '608', '704', '608', '840', '496', '368', '520', '368', '480', '376', '536', '376', '648', '872', '656', '872', '600', '856', '352', '872', '352', '872', '408', '856'], ['512', '376', '512', '424', '456', '416', '376', '360', '352', '272', '560', '416', '640', '368', '680', '288', '504', '592', '472', '592', '384', '704', '400', '848', '544', '592', '624', '704', '616', '840', '496', '360', '520', '360', '480', '368', '536', '368', '656', '872', '656', '864', '608', '856', '352', '872', '352', '872', '408', '856']], [['504', '480', '504', '512', '456', '512', '368', '464', '336', '384', '560', '512', '640', '464', '680', '384', '512', '664', '472', '656', '384', '720', '400', '848', '544', '656', '608', '728', '608', '848', '496', '464', '520', '464', '480', '472', '536', '464', '656', '872', '656', '872', '600', '856', '352', '872', '352', '872', '408', '856'], ['504', '344', '504', '384', '456', '384', '384', '304', '352', '240', '552', '376', '624', '312', '664', '248', '504', '544', '472', '544', '448', '672', '400', '848', '544', '552', '568', '680', '608', '840', '488', '328', '512', '328', '480', '336', '536', '328', '656', '872', '656', '872', '600', '856', '656', '872', '648', '872', '408', '856']], [['504', '584', '504', '592', '456', '600', '376', '568', '336', '496', '560', '592', '632', '552', '688', '480', '504', '744', '472', '736', '392', '728', '400', '848', '536', '744', '600', '728', '608', '848', '496', '576', '520', '576', '480', '568', '536', '560', '648', '872', '648', '864', '600', '856', '352', '872', '352', '872', '408', '856'], ['504', '344', '504', '376', '456', '376', '384', '312', '344', '240', '560', '376', '632', '320', '672', '248', '504', '552', '472', '552', '416', '728', '400', '848', '536', '552', '568', '664', '608', '840', '496', '328', '520', '328', '480', '328', '536', '328', '352', '872', '352', '872', '600', '856', '648', '872', '640', '872', '408', '856']], [['504', '456', '504', '488', '456', '488', '376', '440', '336', '368', '560', '488', '632', '440', '680', '368', '504', '640', '464', '640', '392', '720', '400', '848', '536', '640', '608', '720', '616', '840', '496', '448', '520', '448', '480', '448', '536', '448', '648', '872', '656', '864', '608', '856', '352', '872', '352', '872', '408', '848'], ['496', '456', '504', '488', '448', '480', '368', '432', '336', '344', '552', '488', '632', '432', '680', '360', '504', '648', '464', '648', '376', '720', '400', '848', '544', '648', '616', '728', '608', '848', '488', '440', '512', '440', '472', '440', '528', '440', '656', '872', '656', '872', '600', '856', '352', '872', '344', '872', '408', '856']], [['504', '328', '504', '376', '456', '376', '376', '320', '344', '232', '552', '376', '624', '320', '664', '240', '504', '544', '472', '544', '440', '680', '400', '848', '544', '552', '560', '664', '608', '840', '488', '320', '512', '320', '480', '320', '528', '320', '352', '872', '352', '872', '600', '856', '656', '864', '640', '872', '408', '856'], ['496', '592', '496', '608', '448', '616', '360', '584', '320', '512', '552', '608', '632', '568', '680', '496', '504', '760', '472', '752', '392', '728', '400', '848', '536', '752', '600', '728', '608', '848', '488', '576', '512', '576', '472', '576', '528', '576', '648', '872', '648', '864', '600', '856', '352', '872', '352', '872', '408', '856']], [['504', '336', '504', '376', '456', '376', '376', '320', '336', '232', '560', '368', '632', '312', '672', '232', '504', '544', '472', '544', '424', '720', '400', '848', '536', '544', '560', '664', '608', '840', '488', '328', '512', '328', '480', '328', '528', '320', '656', '872', '656', '872', '600', '856', '656', '872', '648', '872', '408', '856'], ['504', '464', '504', '488', '456', '488', '368', '448', '328', '360', '560', '488', '640', '440', '688', '368', '504', '648', '464', '640', '376', '720', '400', '848', '536', '640', '616', '720', '616', '848', '496', '448', '520', '448', '480', '448', '536', '448', '656', '872', '656', '872', '608', '856', '344', '880', '344', '872', '408', '848']], [['504', '456', '504', '480', '456', '480', '368', '432', '328', '344', '560', '472', '632', '424', '680', '344', '504', '640', '472', '632', '384', '720', '400', '848', '544', '640', '608', '720', '608', '848', '488', '440', '512', '440', '472', '440', '528', '440', '656', '872', '656', '872', '600', '856', '344', '880', '344', '872', '408', '848'], ['504', '336', '504', '376', '456', '376', '368', '288', '344', '224', '552', '376', '624', '312', '672', '248', '504', '544', '472', '544', '440', '688', '400', '848', '536', '552', '560', '680', '608', '840', '488', '320', '512', '320', '480', '328', '528', '320', '352', '872', '352', '872', '600', '856', '656', '872', '648', '872', '408', '856']], [['496', '320', '504', '400', '448', '400', '400', '480', '352', '560', '560', '400', '608', '480', '656', '552', '504', '560', '464', '560', '440', '704', '400', '848', '536', '560', '568', '704', '608', '840', '488', '312', '512', '304', '472', '328', '528', '320', '656', '872', '656', '872', '608', '856', '352', '872', '352', '872', '408', '856'], ['504', '328', '504', '408', '448', '400', '400', '488', '352', '560', '560', '408', '608', '480', '648', '552', '504', '560', '472', '560', '440', '712', '400', '848', '544', '560', '568', '704', '608', '840', '488', '312', '512', '312', '480', '328', '536', '328', '656', '872', '656', '872', '600', '856', '344', '880', '352', '872', '408', '856']]]\n"
     ]
    }
   ],
   "source": [
    "#split the movements in the file into 10 reps (240 frames/arrays)\n",
    "#estimating by dividing by 10 for now to seperate the reps\n",
    "#future me note: can divide each rep by the local minimum of the hip value within x number of frames to obtain the full repinfo\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def chunks(xs):\n",
    "    # number of values to keep in each chunk, rounded down\n",
    "    n = int(len(xs)/10) #nframes/10\n",
    "    \n",
    "    chunk = [] # list of 10 lists, first 9 with n values, 10th with all the rest\n",
    "    for i in range(0, 9):\n",
    "        chunk.append(xs[i * n:(i+1) * n])\n",
    "        \n",
    "    #chunk.append(xs[9 * n:len(xs)])\n",
    "    chunk.append(xs[9 * n:10 * n])\n",
    "    return chunk\n",
    "\n",
    "#load file into a single array (all 240 arrays into 1)\n",
    "f = open(\"DeepSquatJoint.csv\", 'r', newline='')\n",
    "w = csv.reader(f, delimiter = ',', quoting = csv.QUOTE_NONE)\n",
    "\n",
    "raw_data = []\n",
    "\n",
    "for row in w:\n",
    "    raw_data.append(row)\n",
    "    \n",
    "split_data = chunks(raw_data)\n",
    "\n",
    "print(len(split_data))\n",
    "print(split_data)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9c3eb562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2, 50)\n",
      "[[[ 0.00101911 -0.0733758   0.         ... -0.18343949 -0.00917197\n",
      "   -0.02547771]\n",
      "  [ 0.          0.17121019  0.         ... -0.00917197  0.\n",
      "    0.00101911]]\n",
      "\n",
      " [[ 0.00101911 -0.09375796  0.         ...  0.02038217  0.00101911\n",
      "    0.00509554]\n",
      "  [ 0.01019108  0.12025478  0.         ...  0.00101911  0.\n",
      "    0.00101911]]\n",
      "\n",
      " [[ 0.00101911 -0.02242038  0.         ...  0.02038217  0.00101911\n",
      "    0.00509554]\n",
      "  [ 0.01019108 -0.0733758   0.01019108 ...  0.00101911  0.\n",
      "    0.00101911]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.00101911 -0.08356688  0.         ...  0.02038217  0.00101911\n",
      "    0.00509554]\n",
      "  [ 0.          0.03872611  0.         ...  0.00101911  0.\n",
      "   -0.00917197]]\n",
      "\n",
      " [[ 0.00101911  0.06929936  0.         ...  0.02038217  0.00101911\n",
      "   -0.00509554]\n",
      "  [ 0.         -0.12433121  0.         ...  0.00101911  0.\n",
      "    0.00101911]]\n",
      "\n",
      " [[-0.00917197 -0.10394904  0.         ...  0.02038217  0.00101911\n",
      "    0.00509554]\n",
      "  [ 0.         -0.13452229  0.         ...  0.00101911  0.\n",
      "    0.00101911]]]\n"
     ]
    }
   ],
   "source": [
    "#this step is applied after every subject has been analyzed by openpose and split by episode\n",
    "\n",
    "combined = np.array(split_data * 10)\n",
    "combined[combined == ''] = 0\n",
    "combined = combined.astype(float)\n",
    "print(combined.shape)\n",
    "#print(combined)\n",
    "\n",
    "#get the mean of every first value \n",
    "data_mean = np.mean(combined, axis = 0)\n",
    "#Data_mean = repmat(mean(Correct_Xm,2), 1, size(Correct_Xm,2));\n",
    "\n",
    "centered_data = combined - data_mean\n",
    "\n",
    "# Scale the data between -1 and 1\n",
    "scaling_value = np.ceil(max(np.max(centered_Correct_Data), abs(np.min(centered_Correct_Data))))\n",
    "data_correct = centered_data/scaling_value;\n",
    "print(data_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374524bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
