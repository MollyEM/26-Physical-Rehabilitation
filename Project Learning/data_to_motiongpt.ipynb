{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4940384a-69cc-4b9a-a6ed-01b2af422558",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot open camera\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\caffe\\caffe_io.cpp:1138: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"pose_iter_584000.caffemodel\" in function 'cv::dnn::ReadProtoFromBinaryFile'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m humanmlframes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#load the model\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m net \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mreadNetFromCaffe(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpose_deploy.prototxt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpose_iter_584000.caffemodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mframestep\u001b[39m(cap):\n\u001b[0;32m     34\u001b[0m     \n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# Get video properties\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     fps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(cap\u001b[38;5;241m.\u001b[39mget(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_FPS))\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\caffe\\caffe_io.cpp:1138: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"pose_iter_584000.caffemodel\" in function 'cv::dnn::ReadProtoFromBinaryFile'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import json\n",
    "# Open the video file\n",
    "video_path = 'DeepSquat1 (1).avi'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "\n",
    "# Define the keypoint mapping for this OpenPose body_25 model\n",
    "keypoints_mapping = {\n",
    "    0:  \"Nose\", 1:  \"Neck\", 2:  \"RShoulder\", 3:  \"RElbow\", 4:  \"RWrist\", 5:  \"LShoulder\", 6:  \"LElbow\",\n",
    "    7:  \"LWrist\", 8:  \"MidHip\", 9:  \"RHip\", 10: \"RKnee\", 11: \"RAnkle\", 12: \"LHip\", 13: \"LKnee\",\n",
    "    14: \"LAnkle\", 15: \"REye\", 16: \"LEye\", 17: \"REar\", 18: \"LEar\", 19: \"LBigToe\", 20: \"LSmallToe\",\n",
    "    21: \"LHeel\", 22: \"RBigToe\", 23: \"RSmallToe\", 24: \"RHeel\", 25: \"Background\"\n",
    "}\n",
    "\n",
    "humanML_mapping = {\n",
    "    0: \"root\", 1: \"RH\", 2: \"LH\", 3: \"BP\", 4: \"RK\", 5: \"LK\", 6: \"BT\", \n",
    "    7: \"RMrot\", 8: \"LMrot\", 9: \"BLN\", 10: \"RF\", 11: \"LF\", 12: \"BMN\", 13: \"RSI\", \n",
    "    14: \"LSI\", 15: \"BUN\", 16: \"RS\", 17: \"LS\", 18: \"RE\", 19: \"LE\", 20: \"RW\", 21: \"LW\"\n",
    "}\n",
    "\n",
    "#compatible_mapping = [12, 9, 16, 18, 20, 17, 19, 21, 0, 1, 4, x, 2, 5, x, X, X, X, X, X, X, 8, x, x, 7, x]\n",
    "\n",
    "humanml_from_openpose = [8, 4, 7, 8, 10, 13, 8, 24, 21, 1, 22, 19, 0, 2, 5, 0, 2, 5, 3, 6, 4, 7] #check BMN, hand \n",
    "\n",
    "humanmlframes = []\n",
    "\n",
    "#load the model\n",
    "net = cv2.dnn.readNetFromCaffe('pose_deploy.prototxt', 'pose_iter_584000.caffemodel')\n",
    "\n",
    "def framestep(cap):\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    print(\"fps: %d\" %(fps))\n",
    "    print()\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    new_vid = []\n",
    "    if (fps % 10 == 0): #we can easily do 10 fps\n",
    "        stepsize = fps/1 # reset to 10\n",
    "        print(\"total frames to be analyzed: \", frame_count / stepsize)\n",
    "        curr_frame = 0\n",
    "        i = 0\n",
    "        while curr_frame < frame_count:\n",
    "            ret, frame = cap.read()\n",
    "            if curr_frame % stepsize == 0:\n",
    "                print(\"starting frame: \", i)\n",
    "                new_vid.append(pose(frame))\n",
    "                i+=1\n",
    "            curr_frame+=1\n",
    "    \n",
    "    else:\n",
    "        print(\"NOT BUILT YET\")\n",
    "        pass\n",
    "    \n",
    "    return new_vid\n",
    "    \n",
    "def squarify(frame):\n",
    "    height, width, _ = frame.shape\n",
    "    min_dim = min(height, width)\n",
    "\n",
    "    # Calculate the cropping dimensions\n",
    "    crop_height = (height - min_dim) // 2\n",
    "    crop_width = (width - min_dim) // 2\n",
    "\n",
    "    # Crop the image equally from both sides to make it a square\n",
    "    return frame[crop_height:crop_height+min_dim, crop_width:crop_width+min_dim] , min_dim\n",
    "\n",
    "def pose(frame):\n",
    "    \n",
    "    frame, size = squarify(frame)\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255, (size, size),\n",
    "                             (0, 0, 0), swapRB=False, crop=True)\n",
    "\n",
    "    # run forward pass to get the pose estimation\n",
    "    net.setInput(blob)\n",
    "    output = net.forward()\n",
    "    \n",
    "    # Extract joint locations\n",
    "    joint_locations = []\n",
    "\n",
    "    for i in range(len(keypoints_mapping) - 1): #-1 bc we dont want point for the background\n",
    "        keypoint = output[0, i, :, :]\n",
    "        min_val, confidence, min_loc, point = cv2.minMaxLoc(keypoint)\n",
    "\n",
    "        if confidence > 0.1:  # can adjust the confidence threshold if needed ???\n",
    "            joint_locations.append((8 * int(point[0]), 8 * int(point[1]), i))\n",
    "        else:\n",
    "            joint_locations.append(None)\n",
    "\n",
    "    #joint_locations contains the locations of the detected joints and corresponding index\n",
    "    pose_data = {}\n",
    "    for location in joint_locations:\n",
    "        if location:\n",
    "            x, y, index = location\n",
    "            joint_name = keypoints_mapping[index]\n",
    "            pose_data[joint_name] = {'x': x, 'y': y}\n",
    "\n",
    "    return pose_data\n",
    "\n",
    "def process_for_motiongpt(new_vid):\n",
    "    formatted_data = json.dumps(new_vid)\n",
    "    return formatted_data\n",
    "\n",
    "# Process the video frames\n",
    "my_video = framestep(cap)\n",
    "\n",
    "# Prepare the data for MotionGPT\n",
    "formatted_motion_data = process_for_motiongpt(my_video)\n",
    "\n",
    "# Assuming you have a function to feed data into MotionGPT and get results\n",
    "motiongpt_results = motiongpt_process(formatted_motion_data)\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5bed23-db3c-47bb-a0e8-d81321f9f1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c027d43d-6871-4c2f-a587-7e6eab7c8db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b489dff-cca8-4cfb-a5e0-d3d4f2ffb6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
