{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fe0d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directions\n",
    "\n",
    "#make sure you have installed cv2 with command: \n",
    "#  pip install opencv-python\n",
    "#download the file called pose_iter_584000.caffemodel which is saved in this dropbox:\n",
    "#  https://www.dropbox.com/s/3x0xambj2rkyrap/pose_iter_584000.caffemodel?dl=0\n",
    "#download the file called pose_deploy.prototxt which is saved here:\n",
    "#  https://github.com/CMU-Perceptual-Computing-Lab/openpose/tree/master/models/pose/body_25\n",
    "\n",
    "#save both of these files in the same folder as this ipynb\n",
    "#save a video in the same folder or subfolder\n",
    "\n",
    "#issues:\n",
    "\n",
    "# the model keeps recognizing exercise equipment in the background as body parts,\n",
    "# i think using a different model(for multiple people simultaneously), \n",
    "# or adjusting confidence could help with this\n",
    "\n",
    "# i could only make the scaling work by cropping the images to squares first\n",
    "# not a big deal now with our training data, but it could cause problems in the future\n",
    "\n",
    "# i think most of these .avi videos are 30 fps. if not, framestep() will need to be updated\n",
    "\n",
    "# this script does not save the video, it only displays it in a popup window. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "511136c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import csv\n",
    "\n",
    "# For GPU server\n",
    "#path = '/lfs/mead9103.ui/NN_Training/'\n",
    "path = ''\n",
    "\n",
    "def video_to_CSV(filename, frames = 240):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(filename)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Cannot open camera for video %s\" % filename)\n",
    "        return\n",
    "\n",
    "    # Define the keypoint mapping for this OpenPose body_25 model\n",
    "    keypoints_mapping = {\n",
    "        0:  \"Nose\", 1:  \"Neck\", 2:  \"RShoulder\", 3:  \"RElbow\", 4:  \"RWrist\", 5:  \"LShoulder\", 6:  \"LElbow\",\n",
    "        7:  \"LWrist\", 8:  \"MidHip\", 9:  \"RHip\", 10: \"RKnee\", 11: \"RAnkle\", 12: \"LHip\", 13: \"LKnee\",\n",
    "        14: \"LAnkle\", 15: \"REye\", 16: \"LEye\", 17: \"REar\", 18: \"LEar\", 19: \"LBigToe\", 20: \"LSmallToe\",\n",
    "        21: \"LHeel\", 22: \"RBigToe\", 23: \"RSmallToe\", 24: \"RHeel\"\n",
    "    }\n",
    "    #load the model\n",
    "    net = cv2.dnn.readNetFromCaffe(path + 'pose_deploy.prototxt', path + 'pose_iter_584000.caffemodel')\n",
    "\n",
    "    #Write frame information of joint locations into a csv file\n",
    "    f = open(filename + \".csv\", 'w', newline='')\n",
    "    w = csv.writer(f)\n",
    "    #for keypoint, label in keypoints_mapping:\n",
    "        #w.write(str(keypoints_mapping.items())\n",
    "    #w.writerow(keypoints_mapping.values())\n",
    "    \n",
    "    def framestep(cap):\n",
    "        # Get video properties\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        new_vid = []\n",
    "        # Keep stepsize as a float, consider someday skipping frame 0 since it's unlikely to be useful\n",
    "        stepsize = (frame_count - 1) / (frames - 1) # subtract 1 to account for frame 0\n",
    "        print(\"Video: %s, frames to be analyzed: %d (%d / %.2f)\" % (filename, frames, frame_count, stepsize))\n",
    "        curr_frame = 0\n",
    "        i = 0 # float index of next frame\n",
    "        \n",
    "        # Loop until the whole video has been read, or the requested number of frames are reached\n",
    "        while curr_frame < frame_count:\n",
    "            ret, frame = cap.read()\n",
    "            if curr_frame == int(i):\n",
    "                print(\"Starting frame: %d (index %.2f)\" % (curr_frame, i))\n",
    "                new_vid.append(pose(frame))\n",
    "                i += stepsize\n",
    "            curr_frame += 1\n",
    "        return new_vid\n",
    "        \n",
    "    def squarify(frame):\n",
    "        height, width, _ = frame.shape\n",
    "        min_dim = min(height, width)\n",
    "\n",
    "        # Calculate the cropping dimensions\n",
    "        crop_height = (height - min_dim) // 2\n",
    "        crop_width = (width - min_dim) // 2\n",
    "\n",
    "        # Crop the image equally from both sides to make it a square\n",
    "        return frame[crop_height:crop_height+min_dim, crop_width:crop_width+min_dim] , min_dim\n",
    "\n",
    "    def pose(frame):\n",
    "        frame, size = squarify(frame)\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1/255, (size, size),\n",
    "                                (0, 0, 0), swapRB=False, crop=True)\n",
    "\n",
    "        # run forward pass to get the pose estimation\n",
    "        net.setInput(blob)\n",
    "        output = net.forward()\n",
    "        \n",
    "        # Extract joint locations\n",
    "        joint_locations = []\n",
    "        csv_joint_locations = []\n",
    "\n",
    "        for i in range(len(keypoints_mapping)): #-1 bc we dont want point for the background\n",
    "            keypoint = output[0, i, :, :]\n",
    "            min_val, confidence, min_loc, point = cv2.minMaxLoc(keypoint)\n",
    "\n",
    "            #if confidence > 0.1:  # can adjust the confidence threshold if needed ???\n",
    "            joint_locations.append((8 * int(point[0]), 8 * int(point[1]), 0)) #for testing/human readable\n",
    "            csv_joint_locations.append(8 * int(point[0])) #to be printed into csv\n",
    "            csv_joint_locations.append(8 * int(point[1])) #to be printed into csv\n",
    "            csv_joint_locations.append(0) #to be printed into csv\n",
    "            '''else:\n",
    "                joint_locations.append(None) #for testing/human readable\n",
    "                csv_joint_locations.append(8 * int(point[0])) #to be printed into csv\n",
    "                csv_joint_locations.append(8 * int(point[1])) #to be printed into csv\n",
    "            '''\n",
    "        #joint_locations contains the locations of the detected joints and corresponding index\n",
    "\n",
    "        '''for location in joint_locations:\n",
    "            if location:\n",
    "                x, y, index = location\n",
    "                cv2.circle(frame, (x, y), 5, (0, 0, 255), -1)\n",
    "                #cv2.putText(image, str(index), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        '''        \n",
    "        #print joint locations into a csv file\n",
    "        w.writerow(csv_joint_locations)\n",
    "        \n",
    "        return frame\n",
    "\n",
    "    #write the information + circles onto each frame and pop up a window for each frame\n",
    "    my_video = framestep(cap)\n",
    "    size = my_video[0].shape[1], my_video[0].shape[0]\n",
    "    print(size)\n",
    "    '''out = cv2.VideoWriter(\"real.avi\", cv2.VideoWriter_fourcc(*'DIVX'), 30, size)\n",
    "\n",
    "    for i in range(len(my_video)):\n",
    "        out.write(my_video[i])\n",
    "        cv2.imshow(\"video\", my_video[i])\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    out.release()\n",
    "    '''\n",
    "    cap.release()\n",
    "    f.close()\n",
    "\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ad567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to write the frames into a video format for viewer presentation\n",
    "'''\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "def test():\n",
    "    size = (1080, 1080)\n",
    "    out = cv2.VideoWriter(\"deep.avi\", cv2.VideoWriter_fourcc(*'DIVX'), 30, size)\n",
    "    for i in range(0, 200):\n",
    "        f, s = squarify(cap.read()[1])\n",
    "        s\n",
    "        out.write(f)\n",
    "    \n",
    "    out.release()\n",
    "\n",
    "test()\n",
    "cap.release()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9f3f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the movements in the file into 10 reps (240 frames/arrays)\n",
    "#estimating by dividing by 10 for now to seperate the reps\n",
    "#future me note: can divide each rep by the local minimum of the hip value within x number of frames to obtain the full repinfo\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Reads the joint location data from the provided csv file, assuming that the columns are joint positions, and\n",
    "the rows are individual frames. A numpy array is returned with the columns representing frames per episode,\n",
    "and the rows representing joint positions * episodes * subjects.\n",
    "'''\n",
    "def episode_split(filename, frames):\n",
    "    #load file into a single array (all 240 arrays into 1)\n",
    "    with open(filename, 'r', newline='') as f:\n",
    "        w = csv.reader(f, delimiter = ',', quoting = csv.QUOTE_NONE)\n",
    "\n",
    "        raw_data = np.zeros(shape = (frames, 75))\n",
    "\n",
    "        # Populate raw_data with each row representing 1 frame\n",
    "        i = 0\n",
    "        for row in w:\n",
    "            if i >= frames:\n",
    "                break\n",
    "\n",
    "            raw_data[i, :] = row\n",
    "            #print(\"i: \", i, \" row: \", row)\n",
    "            i += 1\n",
    "\n",
    "        #print(raw_data.shape)\n",
    "\n",
    "        raw_data_1 = np.reshape(raw_data, 75 * frames)\n",
    "        #print(raw_data_1)\n",
    "        raw_data_2 = raw_data_1.reshape(75, frames, order = 'F')\n",
    "        #print(raw_data_2)\n",
    "\n",
    "        # Split data into 10 episodes, with an equal number of frames per episode and the last episode getting the remainder\n",
    "\n",
    "        # Number of frames to keep in each episode, rounded down\n",
    "        n = int(frames / 10) # nframes/10\n",
    "\n",
    "        split_data = np.zeros(shape = (75 * 10, n)) # list of 10 lists, first 9 with n frames of joint locations, 10th with all the rest\n",
    "        for i in range(0, 9):\n",
    "            split_data[i * 75:(i + 1) * 75, :] = raw_data_2[:, i * n:(i + 1) * n]\n",
    "\n",
    "        split_data[9 * 75:, :] = raw_data_2[:, 9 * n:]\n",
    "        #split_data.append(raw_data_2[9 * n:len(xs)]) # put the rest of the frames into episode 10\n",
    "\n",
    "        #print(len(split_data))\n",
    "        #print(split_data)\n",
    "        f.close()\n",
    "        return np.array(split_data)\n",
    "\n",
    "#print(episode_split(\"Videos/subject001/DeepSquat1.avi.csv\", 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c3eb562",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: Videos/subject001/DeepSquat1.avi, frames to be analyzed: 3 (520 / 259.50)\n",
      "Starting frame: 0 (index 0.00)\n",
      "Starting frame: 259 (index 259.50)\n",
      "Starting frame: 519 (index 519.00)\n",
      "(1080, 1080)\n",
      "Video: Videos/subject003/DeepSquat1.avi, frames to be analyzed: 3 (496 / 247.50)\n",
      "Starting frame: 0 (index 0.00)\n",
      "Starting frame: 247 (index 247.50)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12248\\1642294301.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#correct data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msubjects\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mvideo_to_CSV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Videos/subject0\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"0\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/DeepSquat1.avi\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframes_to_analyze\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#incorrect data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12248\\1418425977.py\u001b[0m in \u001b[0;36mvideo_to_CSV\u001b[1;34m(filename, frames)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;31m#write the information + circles onto each frame and pop up a window for each frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[0mmy_video\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframestep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m     \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_video\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmy_video\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12248\\1418425977.py\u001b[0m in \u001b[0;36mframestep\u001b[1;34m(cap)\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcurr_frame\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Starting frame: %d (index %.2f)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcurr_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                 \u001b[0mnew_vid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m                 \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mstepsize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mcurr_frame\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12248\\1418425977.py\u001b[0m in \u001b[0;36mpose\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;31m# run forward pass to get the pose estimation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;31m# Extract joint locations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The shortest video has 417 frames\n",
    "frames_to_analyze = 3\n",
    "episodes = 10\n",
    "features = 75\n",
    "\n",
    "subjects = [1, 3, 4, 5, 6, 7, 8, 9, 10, 11] # there is no subject 2\n",
    "\n",
    "#this step is applied after every subject has been analyzed by openpose and split by episode\n",
    "#correct data\n",
    "for i in subjects:\n",
    "    video_to_CSV(path + \"Videos/subject0\" + (\"0\" if i < 10 else '') + str(i) + \"/DeepSquat1.avi\", frames_to_analyze)\n",
    "    \n",
    "#incorrect data\n",
    "for i in subjects:\n",
    "    video_to_CSV(path + \"Videos/subject0\" + (\"0\" if i < 10 else '') + str(i) + \"/DeepSquat2.avi\", frames_to_analyze)\n",
    "\n",
    "#correct data\n",
    "combined = np.zeros(shape = (len(subjects) * episodes * features, frames_to_analyze//10))\n",
    "ind = 0\n",
    "for i in subjects:\n",
    "    combined[ind * episodes * features:(ind + 1) * episodes * features, :] = episode_split(path + \"Videos/subject0\" + (\"0\" if i < 10 else '') + str(i)  + \"/DeepSquat1.avi.csv\", frames_to_analyze)\n",
    "    ind += 1\n",
    "    \n",
    "#incorrect data\n",
    "combined_inc = np.zeros(shape = (len(subjects) * episodes * features, frames_to_analyze//10))\n",
    "ind = 0\n",
    "for i in subjects:\n",
    "    combined_inc[ind * episodes * features:(ind + 1) * episodes * features, :] = episode_split(path + \"Videos/subject0\" + (\"0\" if i < 10 else '') + str(i)  + \"/DeepSquat2.avi.csv\", frames_to_analyze)\n",
    "    ind += 1  \n",
    "\n",
    "combined[combined == ''] = 0\n",
    "combined = combined.astype(float)\n",
    "#print(combined.shape)\n",
    "#print(combined)\n",
    "\n",
    "combined_inc[combined_inc == ''] = 0\n",
    "combined_inc = combined_inc.astype(float)\n",
    "\n",
    "#get the mean of every first value\n",
    "data_mean = np.mean(combined, axis = 0)\n",
    "\n",
    "#inc_data\n",
    "data_mean_inc = np.mean(combined_inc, axis = 0)\n",
    "\n",
    "centered_data = combined - data_mean\n",
    "\n",
    "#inc_data\n",
    "centered_data_inc = combined_inc - data_mean_inc\n",
    "\n",
    "# Scale the data between -1 and 1\n",
    "scaling_value = np.ceil(max(np.max(centered_data), abs(np.min(centered_data))))\n",
    "data_correct = centered_data / scaling_value\n",
    "\n",
    "# Scale the incorrect data between -1 and 1\n",
    "scaling_value_inc = np.ceil(max(np.max(centered_data_inc), abs(np.min(centered_data_inc))))\n",
    "data_incorrect = centered_data_inc / scaling_value_inc\n",
    "\n",
    "#print(len(data_correct))\n",
    "print(data_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17a2676f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of episodes to keep\n",
    "good_indices = [(2, 10), (12, 20), (22, 30), (32, 40), (42, 60), (62, 63), (65, 70), (72, 80), (82, 84), (86, 100)]\n",
    "\n",
    "# Remove the first episodes for most of the subjects due to missing values during the data recording and also remove a few other inconsistent episodes\n",
    "data_correct_red = np.zeros(shape = (90 * features, len(data_correct[0])));\n",
    "i = 0 # number of episodes copied so far\n",
    "for ind in good_indices:\n",
    "    eps = ind[1] - ind[0] + 1 # number of episodes to copy for this range\n",
    "    \n",
    "    # Copy all of the features for every episode within this index's range\n",
    "    data_correct_red[i * features:(i + eps) * features, :] = data_correct[(ind[0] - 1) * features:ind[1] * features, :]\n",
    "    \n",
    "    i += eps # record the number of episodes copied so far\n",
    "    #print(\"episodes this time: \", eps, \" total episodes so far: \", i)\n",
    "    \n",
    "#print(data_correct_red[6600:6750, :])\n",
    "\n",
    "\n",
    "# Remove the first episodes for most of the subjects due to missing values during the data recording and also remove a few other inconsistent episodes\n",
    "data_incorrect_red = np.zeros(shape = (90 * features, len(data_incorrect[0])));\n",
    "i = 0 # number of episodes copied so far\n",
    "for ind in good_indices:\n",
    "    eps = ind[1] - ind[0] + 1 # number of episodes to copy for this range\n",
    "    \n",
    "    # Copy all of the features for every episode within this index's range\n",
    "    data_incorrect_red[i * features:(i + eps) * features, :] = data_incorrect[(ind[0] - 1) * features:ind[1] * features, :]\n",
    "    \n",
    "    i += eps # record the number of episodes copied so far\n",
    "    #print(\"episodes this time: \", eps, \" total episodes so far: \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "374524bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(path + 'data_correct.csv', 'w', newline = '')\n",
    "w = csv.writer(f)\n",
    "np.apply_along_axis(w.writerow, axis = 1, arr = data_correct_red)\n",
    "f.close()\n",
    "\n",
    "\n",
    "f = open(path + 'data_incorrect.csv', 'w', newline = '')\n",
    "w = csv.writer(f)\n",
    "np.apply_along_axis(w.writerow, axis = 1, arr = data_incorrect_red)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af783a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
