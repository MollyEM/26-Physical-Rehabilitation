{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network predicts quality scores for the individual repetitions for Exercise 1 - Deep Squat. The input to the network are the raw measurement data with 75 dimensions. This model is trained on the videos used in the UI-PRMD database. And tested with our own videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Molly Meadows\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and functions\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "import csv\n",
    "import os,random\n",
    "\n",
    "# The code is run on a CPU\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, LSTM, Dense, Dropout, Activation, Flatten, concatenate, UpSampling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import *\n",
    "from keras.layers import Lambda\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "import datetime\n",
    "now = datetime.datetime.now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 40  # number of timesteps\n",
    "nr = 86   # number of repetitions\n",
    "n_dim = 75  # dimension of the data sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Molly Meadows/OneDrive - University of Idaho/Documents/2023-2024/Capstone Project/26-Physical-Rehabilitation/Retest of videos/same_size_csvs/reshaped_data_2_correct.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32436\\852078920.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#Correct_data, Correct_label, Incorrect_data, Incorrect_label = DataViconLoad.load_data()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:/Users/Molly Meadows/OneDrive - University of Idaho/Documents/2023-2024/Capstone Project/26-Physical-Rehabilitation/Retest of videos/same_size_csvs/reshaped_data_2_correct.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquoting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQUOTE_NONE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mCorrect_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Molly Meadows/OneDrive - University of Idaho/Documents/2023-2024/Capstone Project/26-Physical-Rehabilitation/Retest of videos/same_size_csvs/reshaped_data_2_correct.csv'"
     ]
    }
   ],
   "source": [
    "# Import the data\n",
    "#Correct_data, Correct_label, Incorrect_data, Incorrect_label = DataViconLoad.load_data()\n",
    "f = open(r'C:/Users/Molly Meadows/OneDrive - University of Idaho/Documents/2023-2024/Capstone Project/26-Physical-Rehabilitation/Retest of videos/same_size_csvs/reshaped_data_2_correct.csv', 'r')\n",
    "c = csv.reader(f, delimiter = ',', quoting = csv.QUOTE_NONE)\n",
    "Correct_data = np.zeros(shape = (nr, timesteps, n_dim))\n",
    "\n",
    "# read correct data into array\n",
    "i = 0\n",
    "e = 0\n",
    "for row in c:\n",
    "    if e >= 86:\n",
    "        break\n",
    "    Correct_data[e, :, i % 75] = row\n",
    "    i += 1\n",
    "    if i % n_dim == 0:\n",
    "        e += 1 # episode is done when all 75 features have been read\n",
    "\n",
    "f.close()\n",
    "\n",
    "f = open(r'C:/Users/Molly Meadows/OneDrive - University of Idaho/Documents/2023-2024/Capstone Project/26-Physical-Rehabilitation/Retest of videos/same_size_csvs/reshaped_data_2_incorrect.csv', 'r')\n",
    "c = csv.reader(f, delimiter = ',', quoting = csv.QUOTE_NONE)\n",
    "Incorrect_data = np.zeros(shape = (nr, timesteps, n_dim))\n",
    "\n",
    "# read incorrect data into array\n",
    "i = 0\n",
    "e = 0\n",
    "for row in c:\n",
    "    if e >= 86:\n",
    "        break\n",
    "    Incorrect_data[e, :, i % 75] = row\n",
    "    i += 1\n",
    "    if i % n_dim == 0:\n",
    "        e += 1\n",
    "        \n",
    "f.close()\n",
    "\n",
    "f = open(r'C:/Users/Molly Meadows/OneDrive - University of Idaho/Documents/2023-2024/Capstone Project/26-Physical-Rehabilitation/Retest of videos/Model/DataForFinalModel/Labels_Correct_Final.csv', 'r')\n",
    "c = csv.reader(f, delimiter = ',', quoting = csv.QUOTE_NONE)\n",
    "Correct_label = np.zeros(shape = (nr, 1))\n",
    "\n",
    "# read correct labels into array\n",
    "i = 0\n",
    "for row in c:\n",
    "    Correct_label[i, :] = row\n",
    "    i += 1\n",
    "    \n",
    "f.close()\n",
    "\n",
    "f = open(r'C:/Users/Molly Meadows/OneDrive - University of Idaho/Documents/2023-2024/Capstone Project/26-Physical-Rehabilitation/Retest of videos/Model/DataForFinalModel/Labels_Incorrect_Final.csv', 'r')\n",
    "c = csv.reader(f, delimiter = ',', quoting = csv.QUOTE_NONE)\n",
    "Incorrect_label = np.zeros(shape = (nr, 1))\n",
    "\n",
    "# read correct labels into array\n",
    "i = 0\n",
    "for row in c:\n",
    "    Incorrect_label[i, :] = row\n",
    "    i += 1\n",
    "       \n",
    "#Incorrect_label = np.power(Incorrect_label, 3)\n",
    "\n",
    "f.close()\n",
    "\n",
    "# Print the size of the data \n",
    "print(Correct_data.shape, 'correct sequences')\n",
    "print(Correct_label.shape, 'correct labels')\n",
    "print(Incorrect_data.shape, 'incorrect sequences')\n",
    "print(Incorrect_label.shape, 'incorrect labels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-center the data to obtain zero mean\n",
    "'''for i in range(86):\n",
    "    Correct_data[i] = Correct_data[i] - np.mean(Correct_data[i], axis=0)\n",
    "    Incorrect_data[i] = Incorrect_data[i] - np.mean(Incorrect_data[i], axis=0)\n",
    "'''    \n",
    "'''    \n",
    "# Scale the data between -1 and 1\n",
    "#scaling_value = np.ceil(max(np.max(Correct_data), abs(np.min(Correct_data))))\n",
    "scaling_value = np.ceil(np.max(np.abs([np.max(Correct_data), np.min(Correct_data)])))\n",
    "Correct_data = Correct_data / scaling_value\n",
    "\n",
    "#scaling_value = np.ceil(max(np.max(Incorrect_data), abs(np.min(Incorrect_data))))\n",
    "scaling_value = np.ceil(np.max(np.abs([np.max(Incorrect_data), np.min(Incorrect_data)])))\n",
    "Incorrect_data = Incorrect_data / scaling_value\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "# Training set: 70%\n",
    "# Validation set: 30%\n",
    "\n",
    "# Sample random indices\n",
    "trainidx1 = random.sample(range(0, Correct_data.shape[0]), int(nr * 0.7))\n",
    "trainidx2 = random.sample(range(0, Incorrect_data.shape[0]), int(nr * 0.7))\n",
    "valididx1 = np.setdiff1d(np.arange(0, nr, 1), trainidx1)\n",
    "valididx2 = np.setdiff1d(np.arange(0, nr, 1), trainidx2)\n",
    "\n",
    "# Training set: data and labels\n",
    "train_x = np.concatenate((Correct_data[trainidx1, :, :], Incorrect_data[trainidx2, :, :]))\n",
    "print(train_x.shape, 'training data')\n",
    "train_y = np.concatenate((np.squeeze(Correct_label[trainidx1]), np.squeeze(Incorrect_label[trainidx2])))\n",
    "print(train_y.shape, 'training labels')\n",
    "\n",
    "# Validation set: data and labels\n",
    "valid_x = np.concatenate((Correct_data[valididx1, :, :],Incorrect_data[valididx2, :, :]))\n",
    "print(valid_x.shape, 'validation data')\n",
    "valid_y = np.concatenate((np.squeeze(Correct_label[valididx1]), np.squeeze(Incorrect_label[valididx2])))\n",
    "print(valid_y.shape, 'validation labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the first and last sequence in the training and validation sets\n",
    "plt.figure(figsize = (14,4))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(train_x[0])\n",
    "plt.title('First Train Sequence')\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(train_x[-1])\n",
    "plt.title('Last Train Sequence')\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(valid_x[0])\n",
    "plt.title('First Validation Sequence')\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(valid_x[-1])\n",
    "plt.title('Last Validation Sequence')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the data length by a factor of 2, 4, and 8 \n",
    "# The reduced sequences will be used as inputs to the temporal pyramid subnetwork\n",
    "train_x_2 = np.zeros((train_x.shape[0], int(train_x.shape[1]/2), train_x.shape[2]))\n",
    "valid_x_2 = np.zeros(train_x_2.shape)\n",
    "train_x_4 = np.zeros((train_x.shape[0], int(train_x.shape[1]/4), train_x.shape[2]))\n",
    "valid_x_4 = np.zeros(train_x_4.shape)\n",
    "train_x_8 = np.zeros((train_x.shape[0], int(train_x.shape[1]/8), train_x.shape[2]))\n",
    "valid_x_8 = np.zeros(train_x_8.shape)\n",
    "train_x_2 = train_x[:,::2,:]\n",
    "valid_x_2 = valid_x[:,::2,:]\n",
    "train_x_4 = train_x[:,::4,:]\n",
    "valid_x_4 = valid_x[:,::4,:]\n",
    "train_x_8 = train_x[:,::8,:]\n",
    "valid_x_8 = valid_x[:,::8,:]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to re-order the 117 dimensional skeleton data from the Vicon optical tracker into trunk, left arm, right arm, left leg and right leg\n",
    "def reorder_data(x):\n",
    "    X_trunk = np.zeros((x.shape[0], x.shape[1], 27))\n",
    "    X_left_arm = np.zeros((x.shape[0], x.shape[1], 9))\n",
    "    X_right_arm = np.zeros((x.shape[0], x.shape[1], 9))\n",
    "    X_left_leg = np.zeros((x.shape[0], x.shape[1], 18))\n",
    "    X_right_leg = np.zeros((x.shape[0], x.shape[1], 18))\n",
    "\n",
    "   \n",
    "    #X_trunk =  np.concatenate((x[:,:,15:18], x[:,:,6:9]), axis = 2)\n",
    "    #trunk = Lear, Rear, Leye, Reye, nose, neck, LShold, RShoulder, MidHip\n",
    "    X_trunk = np.concatenate((x[:,:,54:57], x[:,:,51:54], x[:,:,48:51], x[:,:,45:48], x[:,:,0:3], x[:,:,3:6], x[:,:,15:18], x[:,:,6:9], x[:,:,24:27]), axis = 2)\n",
    "    \n",
    "    #No radius, Upperhand or hand joints for arms\n",
    "    #LShould, LElbow, LWrist\n",
    "    X_left_arm = np.concatenate((x[:,:,15:18], x[:,:,18:21], x[:,:,21:24]), axis = 2)\n",
    "    #RShould, RElbow, RWirst\n",
    "    X_right_arm = np.concatenate((x[:,:,6:9], x[:,:,9:12], x[:,:,12:15]), axis = 2)  \n",
    "\n",
    "    #no right/left pelivis, femur, tibia for legs\n",
    "    #LHip, LKnee, LAnkle, LHeel\n",
    "    X_left_leg = np.concatenate((x[:,:,36:39], x[:,:,39:42], x[:,:,42:45], x[:,:,63:66], x[:,:,57:60], x[:,:,60:63]), axis = 2)\n",
    "    #RHip, RKnee, RAnkle, RHeel\n",
    "    X_right_leg = np.concatenate((x[:,:,27:30], x[:,:,30:33], x[:,:,33:36], x[:,:,72:75], x[:,:,66:69], x[:,:,69:72]), axis = 2)\n",
    "    x_segmented = np.concatenate((X_trunk, X_right_arm, X_left_arm, X_right_leg, X_left_leg), axis = -1)\n",
    "    return x_segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the data dimensions to correspond to the five body parts\n",
    "trainx =  reorder_data(train_x)\n",
    "validx =  reorder_data(valid_x)\n",
    "trainx_2 =  reorder_data(train_x_2)\n",
    "validx_2 =  reorder_data(valid_x_2)\n",
    "trainx_4 =  reorder_data(train_x_4)\n",
    "validx_4 =  reorder_data(valid_x_4)\n",
    "trainx_8 =  reorder_data(train_x_8)\n",
    "validx_8 =  reorder_data(valid_x_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a multibranch convolutional Inception-like block\n",
    "def MultiBranchConv1D(input, filters1, kernel_size1, strides1, strides2):\n",
    "    x1 = Conv1D(filters=filters1, kernel_size=kernel_size1+2, strides=strides1, padding='same', activation='relu')(input)\n",
    "    x1 = Dropout(0.25)(x1)\n",
    "    x2 = Conv1D(filters=filters1, kernel_size=kernel_size1+6, strides=strides1, padding='same', activation='relu')(input)\n",
    "    x2 = Dropout(0.25)(x2)\n",
    "    x3 = Conv1D(filters=filters1, kernel_size=kernel_size1+12, strides=strides1, padding='same', activation='relu')(input)\n",
    "    x3 = Dropout(0.25)(x3)\n",
    "    y1 = concatenate([x1, x2, x3], axis=-1)\n",
    "\n",
    "    x4 = Conv1D(filters=filters1, kernel_size=kernel_size1, strides=strides2, padding='same', activation='relu')(y1)\n",
    "    x4 = Dropout(0.25)(x4)\n",
    "    x5 = Conv1D(filters=filters1, kernel_size=kernel_size1+2, strides=strides2, padding='same', activation='relu')(y1)\n",
    "    x5 = Dropout(0.25)(x5)\n",
    "    x6 = Conv1D(filters=filters1, kernel_size=kernel_size1+4, strides=strides2, padding='same', activation='relu')(y1)\n",
    "    x6 = Dropout(0.25)(x6)\n",
    "    x = concatenate([x4, x5, x6], axis=-1)                                                                                                                                   \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a temporal pyramid network\n",
    "def TempPyramid(input_f, input_2, input_4, input_8, seq_len, n_dims):\n",
    "    \n",
    "    #### Full scale sequences\n",
    "    conv1 = MultiBranchConv1D(input_f, 64, 3, 2, 2)\n",
    "\n",
    "    #### Half scale sequences\n",
    "    conv2 = MultiBranchConv1D(input_2, 64, 3, 2, 1)\n",
    "\n",
    "    #### Quarter scale sequences\n",
    "    conv3 = MultiBranchConv1D(input_4, 64, 3, 1, 1)\n",
    "\n",
    "    #### Eighth scale sequences\n",
    "    conv4 = MultiBranchConv1D(input_8, 64, 3, 1, 1)\n",
    "    upsample1 = UpSampling1D(size = 2)(conv4)\n",
    "\n",
    "    #### Recurrent layers\n",
    "    x = concatenate([conv1, conv2, conv3, upsample1], axis=-1)\n",
    "    return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_dim = 48 # dimension after segmenting the data into body parts\n",
    "#n_dim1 = 6 # trunk dimension\n",
    "n_dim = 81\n",
    "n_dim1 = 27\n",
    "n_dim2 = 9 # arms dimension\n",
    "n_dim3 = 18 # legs dimension\n",
    "\n",
    "# Build the model ...\n",
    "\n",
    "#### Full scale sequences\n",
    "seq_input = Input(shape = (timesteps, n_dim), name = 'full_scale')\n",
    "\n",
    "'''\n",
    "seq_input_trunk = Lambda(lambda x: x[:, :, 0:6])(seq_input)\n",
    "seq_input_left_arm = Lambda(lambda x: x[:, :, 6:15])(seq_input)\n",
    "seq_input_right_arm = Lambda(lambda x: x[:, :, 15:24])(seq_input)\n",
    "seq_input_left_leg = Lambda(lambda x: x[:, :, 24:36])(seq_input)\n",
    "seq_input_right_leg = Lambda(lambda x: x[:, :, 36:48])(seq_input)\n",
    "'''\n",
    "\n",
    "seq_input_trunk = Lambda(lambda x: x[:, :, 0:27])(seq_input)\n",
    "seq_input_left_arm = Lambda(lambda x: x[:, :, 27:36])(seq_input)\n",
    "seq_input_right_arm = Lambda(lambda x: x[:, :, 36:45])(seq_input)\n",
    "seq_input_left_leg = Lambda(lambda x: x[:, :, 45:63])(seq_input)\n",
    "seq_input_right_leg = Lambda(lambda x: x[:, :, 63:81])(seq_input)\n",
    "\n",
    "#### Half scale sequences\n",
    "seq_input_2 = Input(shape=(int(timesteps/2), n_dim), name='half_scale')\n",
    "\n",
    "seq_input_trunk_2 = Lambda(lambda x: x[:, :, 0:27])(seq_input_2)\n",
    "seq_input_left_arm_2 = Lambda(lambda x: x[:, :, 27:36])(seq_input_2)\n",
    "seq_input_right_arm_2 = Lambda(lambda x: x[:, :, 36:45])(seq_input_2)\n",
    "seq_input_left_leg_2 = Lambda(lambda x: x[:, :, 45:63])(seq_input_2)\n",
    "seq_input_right_leg_2 = Lambda(lambda x: x[:, :, 63:81])(seq_input_2)\n",
    "\n",
    "#### Quarter scale sequences\n",
    "seq_input_4 = Input(shape=(int(timesteps/4), n_dim), name='quarter_scale')\n",
    "\n",
    "seq_input_trunk_4 = Lambda(lambda x: x[:, :, 0:27])(seq_input_4)\n",
    "seq_input_left_arm_4 = Lambda(lambda x: x[:, :, 27:36])(seq_input_4)\n",
    "seq_input_right_arm_4 = Lambda(lambda x: x[:, :, 36:45])(seq_input_4)\n",
    "seq_input_left_leg_4 = Lambda(lambda x: x[:, :, 45:63])(seq_input_4)\n",
    "seq_input_right_leg_4 = Lambda(lambda x: x[:, :, 63:81])(seq_input_4)\n",
    "\n",
    "#### Eighth scale sequences\n",
    "seq_input_8 = Input(shape=(int(timesteps/8), n_dim), name='eighth_scale')\n",
    "\n",
    "seq_input_trunk_8 = Lambda(lambda x: x[:, :, 0:27])(seq_input_8)\n",
    "seq_input_left_arm_8 = Lambda(lambda x: x[:, :, 27:36])(seq_input_8)\n",
    "seq_input_right_arm_8 = Lambda(lambda x: x[:, :, 36:45])(seq_input_8)\n",
    "seq_input_left_leg_8 = Lambda(lambda x: x[:, :, 45:63])(seq_input_8)\n",
    "seq_input_right_leg_8 = Lambda(lambda x: x[:, :, 63:81])(seq_input_8)\n",
    "\n",
    "concat_trunk = TempPyramid(seq_input_trunk, seq_input_trunk_2, seq_input_trunk_4, seq_input_trunk_8, timesteps, n_dim1)\n",
    "concat_left_arm = TempPyramid(seq_input_left_arm, seq_input_left_arm_2, seq_input_left_arm_4, seq_input_left_arm_8, timesteps, n_dim2)\n",
    "concat_right_arm = TempPyramid(seq_input_right_arm, seq_input_right_arm_2, seq_input_right_arm_4, seq_input_right_arm_8, timesteps, n_dim2)\n",
    "concat_left_leg = TempPyramid(seq_input_left_leg, seq_input_left_leg_2, seq_input_left_leg_4, seq_input_left_leg_8, timesteps, n_dim3)\n",
    "concat_right_leg = TempPyramid(seq_input_right_leg, seq_input_right_leg_2, seq_input_right_leg_4, seq_input_right_leg_8, timesteps, n_dim3)\n",
    "\n",
    "concat = concatenate([concat_trunk, concat_left_arm, concat_right_arm, concat_left_leg, concat_right_leg], axis=-1)\n",
    "\n",
    "#not sure what these are numbers are from?\n",
    "rec = LSTM(80, return_sequences=True)(concat)\n",
    "rec1 = LSTM(40, return_sequences=True)(rec)\n",
    "rec1 = LSTM(40, return_sequences=True)(rec1)\n",
    "rec2 = LSTM(80)(rec1)\n",
    "\n",
    "out = Dense(1, activation = 'tanh')(rec2)\n",
    "\n",
    "model = Model(inputs=[seq_input, seq_input_2, seq_input_4, seq_input_8], outputs=out)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer= Adam(learning_rate= 0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = now()\n",
    "    \n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience = 25)\n",
    "\n",
    "history = model.fit([trainx, trainx_2, trainx_4, trainx_8], train_y, batch_size = 16, epochs = 500, verbose=0, \n",
    "                validation_data=([validx, validx_2, validx_4, validx_8], valid_y), callbacks = [early_stopping])\n",
    "\n",
    "print('Training time: %s' % (now() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(1)\n",
    "plt.plot(history.history['loss'], 'b', label = 'Training Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.plot(history.history['val_loss'], 'r', label = 'Validation Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the minimum loss\n",
    "print(\"Training loss\", np.min(history.history['loss']))\n",
    "print(\"Validation loss\",np.min(history.history['val_loss']))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the prediction of the model for the training and validation sets\n",
    "pred_train = model.predict([trainx, trainx_2, trainx_4, trainx_8])\n",
    "\n",
    "pred_test = model.predict([validx, validx_2, validx_4, validx_8])\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(pred_train,'s', color='red', label='Prediction', linestyle='None', alpha = 0.5, markersize=6)\n",
    "plt.plot(train_y,'o', color='green',label='Quality Score', alpha = 0.4, markersize=6)\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.title('Training Set',fontsize=14)\n",
    "plt.xlabel('Input Exercise',fontsize=14)\n",
    "plt.ylabel('Quality Scale',fontsize=14)\n",
    "plt.legend(loc=3, prop={'size':14}) # loc:position\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(pred_test,'s', color='red', label='Prediction', linestyle='None', alpha = 0.5, markersize=6)\n",
    "plt.plot(valid_y,'o', color='green',label='Quality Score', alpha = 0.4, markersize=6)\n",
    "plt.title('Testing Set',fontsize=14)\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.xlabel('Input Exercise',fontsize=14)\n",
    "plt.ylabel('Quality Scale',fontsize=14)\n",
    "plt.legend(loc=3, prop={'size':14}) # loc:position\n",
    "plt.tight_layout()\n",
    "#plt.savefig('../../Results/SpatioTemporalNN_Vicon_Scores.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cumulative deviation and rms deviation for the validation set\n",
    "test_dev = abs(np.squeeze(pred_test)-valid_y)\n",
    "# Cumulative deviation\n",
    "mean_abs_dev = np.mean(test_dev)\n",
    "# RMS deviation\n",
    "rms_dev = sqrt(mean_squared_error(pred_test, valid_y))\n",
    "print('Mean absolute deviation:', mean_abs_dev)\n",
    "print('RMS deviation:', rms_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "#Unit test, import videos of self doing exercise and produce accuracy score on trained model\n",
    "def userTest(filename):\n",
    "    \n",
    "    n = 75\n",
    "    \n",
    "    f = open(filename, 'r')\n",
    "    c = csv.reader(f, delimiter = ',', quoting = csv.QUOTE_NONE)\n",
    "    ctrain_x = np.zeros(shape = (1, timesteps, n))\n",
    "    \n",
    "    i = 0\n",
    "    for row in c:\n",
    "        ctrain_x[0, :, i % 75] = row\n",
    "        i += 1\n",
    "\n",
    "    #print(ctrain_x.shape)\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    ctrain_x_2 = np.zeros((ctrain_x.shape[0], int(ctrain_x.shape[1]/2), ctrain_x.shape[2]))\n",
    "    ctrain_x_4 = np.zeros((ctrain_x.shape[0], int(ctrain_x.shape[1]/4), ctrain_x.shape[2]))\n",
    "    ctrain_x_8 = np.zeros((ctrain_x.shape[0], int(ctrain_x.shape[1]/8), ctrain_x.shape[2]))\n",
    "    ctrain_x_2 = ctrain_x[:,::2,:]\n",
    "    ctrain_x_4 = ctrain_x[:,::4,:]\n",
    "    ctrain_x_8 = ctrain_x[:,::8,:] \n",
    "    \n",
    "    #print(ctrain_x_8)\n",
    "\n",
    "    # Reorder the data dimensions to correspond to the five body parts\n",
    "    ctrainx =  reorder_data(ctrain_x)\n",
    "    ctrainx_2 =  reorder_data(ctrain_x_2)\n",
    "    ctrainx_4 =  reorder_data(ctrain_x_4)\n",
    "    ctrainx_8 =  reorder_data(ctrain_x_8) \n",
    "    \n",
    "    #print(ctrainx.shape, ctrain_x_2.shape, ctrain_x_4.shape, ctrain_x_4.shape)\n",
    "\n",
    "    #prediction\n",
    "    pred = model.predict([ctrainx, ctrainx_2, ctrainx_4, ctrainx_8])\n",
    "    #print(type(pred))\n",
    "    print(\"Your accuracy rating for \" + filename + \" is: \", pred)\n",
    "    #print(pred[0, 0])\n",
    "    return pred[0, 0]\n",
    "    #plt.plot(pred,'s', color='red', label='Prediction', linestyle='None', alpha = 0.5, markersize=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(7, 7))\n",
    "\n",
    "#store scores of good squats\n",
    "good = []\n",
    "\n",
    "#directory where testing files are stored\n",
    "dir_path = 'C:/Users/Molly Meadows/Downloads/GoodSquats/'\n",
    "\n",
    "G_squat = []\n",
    "\n",
    "for path in os.listdir(dir_path):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "        G_squat.append(path)\n",
    "        \n",
    "for f in G_squat:\n",
    "    good.append(userTest(dir_path + f))\n",
    "    #print(f)\n",
    "\n",
    "\n",
    "# Creating plot\n",
    "plt.title('Correct Squat Testing',fontsize=14)\n",
    "plt.ylabel('Quality Scale',fontsize=14)\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.boxplot(good, manage_ticks=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userTest(r\"C:\\Users\\Molly Meadows\\OneDrive - University of Idaho\\Documents\\2023-2024\\Capstone Project\\Retest of videos\\same_size_csvs\\MollyGoodSquat_smoothed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(good))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(8, 8))\n",
    "\n",
    "#store scores of good squats\n",
    "bad = []\n",
    "\n",
    "#directory where testing files are stored\n",
    "dir_path = r\"C:\\Users\\Molly Meadows\\OneDrive - University of Idaho\\Documents\\2023-2024\\Capstone Project\\26-Physical-Rehabilitation\\LungeAndShoulderSegmentedVideos\\IncorrectDeepSquatUser\\IncorrectDeepSquatTesting\\Centered\\\\\"\n",
    "B_squat = []\n",
    "\n",
    "for path in os.listdir(dir_path):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "        B_squat.append(path)\n",
    "        \n",
    "for f in B_squat:\n",
    "    bad.append(userTest(dir_path + f))\n",
    "    #print(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Creating plot\n",
    "plt.title('Incorrect Squat Testing',fontsize=14)\n",
    "plt.ylabel('Quality Scale',fontsize=14)\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.boxplot(bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"my_models.OpenPose_SpatioTemporalNN_Vicon_Final_Reshaped_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "test = pd.read_csv(filename, header=None)\n",
    "    test = test.to_numpy()\n",
    "    \n",
    "    test = np.swapaxes(test, 1, 0)\n",
    "\n",
    "    \n",
    "    ctrain_x = np.reshape(test, (1, 40, 75))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
