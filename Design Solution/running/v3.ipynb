{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fe0d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directions\n",
    "\n",
    "#make sure you have installed cv2 with command: \n",
    "#  pip install opencv-python\n",
    "#download the file called pose_iter_584000.caffemodel which is saved in this dropbox:\n",
    "#  https://www.dropbox.com/s/3x0xambj2rkyrap/pose_iter_584000.caffemodel?dl=0\n",
    "#download the file called pose_deploy.prototxt which is saved here:\n",
    "#  https://github.com/CMU-Perceptual-Computing-Lab/openpose/tree/master/models/pose/body_25\n",
    "\n",
    "#save both of these files in the same folder as this ipynb\n",
    "#save a video in the same folder or subfolder\n",
    "\n",
    "\n",
    "#issues:\n",
    "\n",
    "# the model keeps recognizing exercise equipment in the background as body parts,\n",
    "# i think using a different model(for multiple people simultaneously), \n",
    "# or adjusting confidence could help with this\n",
    "\n",
    "# i could only make the scaling work by cropping the images to squares first\n",
    "# not a big deal now with our training data, but it could cause problems in the future\n",
    "\n",
    "# i think most of these .avi videos are 30 fps. if not, framestep() will need to be updated\n",
    "\n",
    "# this script does not save the video, it only displays it in a popup window. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e598c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec84bc49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "511136c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fps: 30\n",
      "\n",
      "total frames to be analyzed:  172.33333333333334\n",
      "starting frame:  0\n",
      "starting frame:  1\n",
      "starting frame:  2\n",
      "starting frame:  3\n",
      "starting frame:  4\n",
      "starting frame:  5\n",
      "starting frame:  6\n",
      "starting frame:  7\n",
      "starting frame:  8\n",
      "starting frame:  9\n",
      "starting frame:  10\n",
      "starting frame:  11\n",
      "starting frame:  12\n",
      "starting frame:  13\n",
      "starting frame:  14\n",
      "starting frame:  15\n",
      "starting frame:  16\n",
      "starting frame:  17\n",
      "starting frame:  18\n",
      "starting frame:  19\n",
      "starting frame:  20\n",
      "starting frame:  21\n",
      "starting frame:  22\n",
      "starting frame:  23\n",
      "starting frame:  24\n",
      "starting frame:  25\n",
      "starting frame:  26\n",
      "starting frame:  27\n",
      "starting frame:  28\n",
      "starting frame:  29\n",
      "starting frame:  30\n",
      "starting frame:  31\n",
      "starting frame:  32\n",
      "starting frame:  33\n",
      "starting frame:  34\n",
      "starting frame:  35\n",
      "starting frame:  36\n",
      "starting frame:  37\n",
      "starting frame:  38\n",
      "starting frame:  39\n",
      "starting frame:  40\n",
      "starting frame:  41\n",
      "starting frame:  42\n",
      "starting frame:  43\n",
      "starting frame:  44\n",
      "starting frame:  45\n",
      "starting frame:  46\n",
      "starting frame:  47\n",
      "starting frame:  48\n",
      "starting frame:  49\n",
      "starting frame:  50\n",
      "starting frame:  51\n",
      "starting frame:  52\n",
      "starting frame:  53\n",
      "starting frame:  54\n",
      "starting frame:  55\n",
      "starting frame:  56\n",
      "starting frame:  57\n",
      "starting frame:  58\n",
      "starting frame:  59\n",
      "starting frame:  60\n",
      "starting frame:  61\n",
      "starting frame:  62\n",
      "starting frame:  63\n",
      "starting frame:  64\n",
      "starting frame:  65\n",
      "starting frame:  66\n",
      "starting frame:  67\n",
      "starting frame:  68\n",
      "starting frame:  69\n",
      "starting frame:  70\n",
      "starting frame:  71\n",
      "starting frame:  72\n",
      "starting frame:  73\n",
      "starting frame:  74\n",
      "starting frame:  75\n",
      "starting frame:  76\n",
      "starting frame:  77\n",
      "starting frame:  78\n",
      "starting frame:  79\n",
      "starting frame:  80\n",
      "starting frame:  81\n",
      "starting frame:  82\n",
      "starting frame:  83\n",
      "starting frame:  84\n",
      "starting frame:  85\n",
      "starting frame:  86\n",
      "starting frame:  87\n",
      "starting frame:  88\n",
      "starting frame:  89\n",
      "starting frame:  90\n",
      "starting frame:  91\n",
      "starting frame:  92\n",
      "starting frame:  93\n",
      "starting frame:  94\n",
      "starting frame:  95\n",
      "starting frame:  96\n",
      "starting frame:  97\n",
      "starting frame:  98\n",
      "starting frame:  99\n",
      "starting frame:  100\n",
      "starting frame:  101\n",
      "starting frame:  102\n",
      "starting frame:  103\n",
      "starting frame:  104\n",
      "starting frame:  105\n",
      "starting frame:  106\n",
      "starting frame:  107\n",
      "starting frame:  108\n",
      "starting frame:  109\n",
      "starting frame:  110\n",
      "starting frame:  111\n",
      "starting frame:  112\n",
      "starting frame:  113\n",
      "starting frame:  114\n",
      "starting frame:  115\n",
      "starting frame:  116\n",
      "starting frame:  117\n",
      "starting frame:  118\n",
      "starting frame:  119\n",
      "starting frame:  120\n",
      "starting frame:  121\n",
      "starting frame:  122\n",
      "starting frame:  123\n",
      "starting frame:  124\n",
      "starting frame:  125\n",
      "starting frame:  126\n",
      "starting frame:  127\n",
      "starting frame:  128\n",
      "starting frame:  129\n",
      "starting frame:  130\n",
      "starting frame:  131\n",
      "starting frame:  132\n",
      "starting frame:  133\n",
      "starting frame:  134\n",
      "starting frame:  135\n",
      "starting frame:  136\n",
      "starting frame:  137\n",
      "starting frame:  138\n",
      "starting frame:  139\n",
      "starting frame:  140\n",
      "starting frame:  141\n",
      "starting frame:  142\n",
      "starting frame:  143\n",
      "starting frame:  144\n",
      "starting frame:  145\n",
      "starting frame:  146\n",
      "starting frame:  147\n",
      "starting frame:  148\n",
      "starting frame:  149\n",
      "starting frame:  150\n",
      "starting frame:  151\n",
      "starting frame:  152\n",
      "starting frame:  153\n",
      "starting frame:  154\n",
      "starting frame:  155\n",
      "starting frame:  156\n",
      "starting frame:  157\n",
      "starting frame:  158\n",
      "starting frame:  159\n",
      "starting frame:  160\n",
      "starting frame:  161\n",
      "starting frame:  162\n",
      "starting frame:  163\n",
      "starting frame:  164\n",
      "starting frame:  165\n",
      "starting frame:  166\n",
      "starting frame:  167\n",
      "starting frame:  168\n",
      "starting frame:  169\n",
      "starting frame:  170\n",
      "starting frame:  171\n",
      "starting frame:  172\n",
      "(1080, 1080)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Open the video file\n",
    "video_path = 'DeepSquat1 (1).avi'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "\n",
    "# Define the keypoint mapping for this OpenPose body_25 model\n",
    "keypoints_mapping = {\n",
    "    0:  \"Nose\", 1:  \"Neck\", 2:  \"RShoulder\", 3:  \"RElbow\", 4:  \"RWrist\", 5:  \"LShoulder\", 6:  \"LElbow\",\n",
    "    7:  \"LWrist\", 8:  \"MidHip\", 9:  \"RHip\", 10: \"RKnee\", 11: \"RAnkle\", 12: \"LHip\", 13: \"LKnee\",\n",
    "    14: \"LAnkle\", 15: \"REye\", 16: \"LEye\", 17: \"REar\", 18: \"LEar\", 19: \"LBigToe\", 20: \"LSmallToe\",\n",
    "    21: \"LHeel\", 22: \"RBigToe\", 23: \"RSmallToe\", 24: \"RHeel\", 25: \"Background\"\n",
    "}\n",
    "#load the model\n",
    "net = cv2.dnn.readNetFromCaffe('pose_deploy.prototxt', 'pose_iter_584000.caffemodel')\n",
    "\n",
    "def framestep(cap):\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    print(\"fps: %d\" %(fps))\n",
    "    print()\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    new_vid = []\n",
    "    if (fps % 10 == 0): #we can easily do 10 fps\n",
    "        stepsize = fps/10 # reset to 10\n",
    "        print(\"total frames to be analyzed: \", frame_count / stepsize)\n",
    "        curr_frame = 0\n",
    "        i = 0\n",
    "        while curr_frame < frame_count:\n",
    "            ret, frame = cap.read()\n",
    "            if curr_frame % stepsize == 0:\n",
    "                print(\"starting frame: \", i)\n",
    "                new_vid.append(pose(frame))\n",
    "                i+=1\n",
    "            curr_frame+=1\n",
    "    \n",
    "    else:\n",
    "        print(\"NOT BUILT YET\")\n",
    "        pass\n",
    "    \n",
    "    return new_vid\n",
    "    \n",
    "def squarify(frame):\n",
    "    height, width, _ = frame.shape\n",
    "    min_dim = min(height, width)\n",
    "\n",
    "    # Calculate the cropping dimensions\n",
    "    crop_height = (height - min_dim) // 2\n",
    "    crop_width = (width - min_dim) // 2\n",
    "\n",
    "    # Crop the image equally from both sides to make it a square\n",
    "    return frame[crop_height:crop_height+min_dim, crop_width:crop_width+min_dim] , min_dim\n",
    "\n",
    "def pose(frame):\n",
    "    \n",
    "    frame, size = squarify(frame)\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255, (size, size),\n",
    "                             (0, 0, 0), swapRB=False, crop=True)\n",
    "\n",
    "    # run forward pass to get the pose estimation\n",
    "    net.setInput(blob)\n",
    "    output = net.forward()\n",
    "    \n",
    "    # Extract joint locations\n",
    "    joint_locations = []\n",
    "\n",
    "    for i in range(len(keypoints_mapping) - 1): #-1 bc we dont want point for the background\n",
    "        keypoint = output[0, i, :, :]\n",
    "        min_val, confidence, min_loc, point = cv2.minMaxLoc(keypoint)\n",
    "\n",
    "        if confidence > 0.1:  # can adjust the confidence threshold if needed ???\n",
    "            joint_locations.append((8 * int(point[0]), 8 * int(point[1]), i))\n",
    "        else:\n",
    "            joint_locations.append(None)\n",
    "\n",
    "    #joint_locations contains the locations of the detected joints and corresponding index\n",
    "\n",
    "    for location in joint_locations:\n",
    "        if location:\n",
    "            x, y, index = location\n",
    "            cv2.circle(frame, (x, y), 5, (0, 0, 255), -1)\n",
    "            #cv2.putText(image, str(index), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "    return frame\n",
    "\n",
    "my_video = framestep(cap)\n",
    "size = my_video[0].shape[1], my_video[0].shape[0]\n",
    "print(size)\n",
    "out = cv2.VideoWriter(\"real.avi\", cv2.VideoWriter_fourcc(*'DIVX'), 30, size)\n",
    "\n",
    "for i in range(len(my_video)):\n",
    "    out.write(my_video[i])\n",
    "    cv2.imshow(\"video\", my_video[i])\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "out.release()\n",
    "cap.release()\n",
    "\n",
    "exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26ad567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "def test():\n",
    "    size = (1080, 1080)\n",
    "    out = cv2.VideoWriter(\"deep.avi\", cv2.VideoWriter_fourcc(*'DIVX'), 30, size)\n",
    "    for i in range(0, 200):\n",
    "        f, s = squarify(cap.read()[1])\n",
    "        s\n",
    "        out.write(f)\n",
    "    \n",
    "    out.release()\n",
    "\n",
    "test()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f3f2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
